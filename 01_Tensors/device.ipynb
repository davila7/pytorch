{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Section: Creating a Tensor\n",
    "\n",
    " PyTorch and handling devices for GPU computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [1, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# import torch allows you to access the functionalities of PyTorch\n",
    "import torch\n",
    "\n",
    "# creates a list of lists in Python, which you can think of as a 2x3 matrix.\n",
    "array = [[1,2,3], [1,5,6]]\n",
    "\n",
    "# tensor1 = torch.tensor(array) takes the array you defined and converts it into a PyTorch tensor. Tensors are the central data structure in PyTorch and are used to store data that can be processed in deep learning operations. In this case, tensor1 is a tensor of size 2x3 containing the same values as the original array.\n",
    "tensor1 = torch.tensor(array)\n",
    "\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a tensor in PyTorch, it is by default allocated on the CPU. If you have a GPU available and you want to perform computations on it, you would typically move the tensor to the GPU using .to('cuda') or .cuda() method. Conversely, you can move a tensor back to the CPU with .to('cpu') or .cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.cuda.is_available() is a function that checks if CUDA is available on your system. CUDA is a parallel computing platform and programming model invented by NVIDIA. It allows for significantly increased computing speeds using the GPU (Graphics Processing Unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") is a conditional expression (also known as a ternary operator) that assigns the string \"cuda\" to the variable device if a CUDA-compatible GPU is available, or the string \"cpu\" if it is not. This means that your computations will be performed on the GPU if possible, which can significantly speed up deep learning operations, or on the CPU if no GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tensor1 = tensor1.to(device)\n",
    "print(tensor1.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
